{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfX4NXaXzIZN"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AnzjwE75c4Kq"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "!pip install emoji\n",
    "import emoji\n",
    "\n",
    "!pip install sastrawi\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a6SUSseYTgHY",
    "outputId": "46760540-fcb7-4692-8c6c-fa59aef69023"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4D7LHNImzMni"
   },
   "source": [
    "### Memuat Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "lQrmCCPSYBam",
    "outputId": "fcc4dbeb-7408-44ca-de12-b82ff5b94a52"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewId</th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>appVersion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1bbb65ec-9126-46e9-a9ed-697b73d563e3</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>bank terramah sepanjang masa</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6.1</td>\n",
       "      <td>2025-04-08 06:18:15</td>\n",
       "      <td>Terima kasih atas ulasannya. Semoga aplikasi B...</td>\n",
       "      <td>2025-04-08 07:37:39</td>\n",
       "      <td>4.6.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b4000e13-6a3b-41de-b865-4eb433d57d09</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>pulsa udah abis banyak buat verifikasi tapi ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-08 06:07:59</td>\n",
       "      <td>Mohon maaf atas ketidaknyamanan Bapak/Ibu, unt...</td>\n",
       "      <td>2025-04-08 07:37:38</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a4923423-2ea1-427d-8af3-59355a454706</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>kode otp tidak masuk masuk tolong perbaiki</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-08 05:10:12</td>\n",
       "      <td>Mohon maaf atas ketidaknyamanan Bapak/Ibu, unt...</td>\n",
       "      <td>2025-04-08 05:20:35</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ccbf6880-a6d8-4366-ba9b-8c6e300c8ea2</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>sangat puas</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025-04-08 05:02:04</td>\n",
       "      <td>Terima kasih atas ulasannya. Semoga aplikasi B...</td>\n",
       "      <td>2025-04-08 05:06:44</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3a5d5110-1abc-42bb-9f6d-cdbe0eda54d1</td>\n",
       "      <td>Pengguna Google</td>\n",
       "      <td>https://play-lh.googleusercontent.com/EGemoI2N...</td>\n",
       "      <td>Bagus</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4.6.1</td>\n",
       "      <td>2025-04-08 04:33:18</td>\n",
       "      <td>Terima kasih atas ulasannya. Semoga aplikasi B...</td>\n",
       "      <td>2025-04-08 05:06:43</td>\n",
       "      <td>4.6.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               reviewId         userName  \\\n",
       "0  1bbb65ec-9126-46e9-a9ed-697b73d563e3  Pengguna Google   \n",
       "1  b4000e13-6a3b-41de-b865-4eb433d57d09  Pengguna Google   \n",
       "2  a4923423-2ea1-427d-8af3-59355a454706  Pengguna Google   \n",
       "3  ccbf6880-a6d8-4366-ba9b-8c6e300c8ea2  Pengguna Google   \n",
       "4  3a5d5110-1abc-42bb-9f6d-cdbe0eda54d1  Pengguna Google   \n",
       "\n",
       "                                           userImage  \\\n",
       "0  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "1  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "2  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "3  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "4  https://play-lh.googleusercontent.com/EGemoI2N...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0                       bank terramah sepanjang masa      5              0   \n",
       "1  pulsa udah abis banyak buat verifikasi tapi ke...      1              0   \n",
       "2         kode otp tidak masuk masuk tolong perbaiki      1              0   \n",
       "3                                        sangat puas      5              0   \n",
       "4                                              Bagus      4              0   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0                4.6.1  2025-04-08 06:18:15   \n",
       "1                  NaN  2025-04-08 06:07:59   \n",
       "2                  NaN  2025-04-08 05:10:12   \n",
       "3                  NaN  2025-04-08 05:02:04   \n",
       "4                4.6.1  2025-04-08 04:33:18   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0  Terima kasih atas ulasannya. Semoga aplikasi B...  2025-04-08 07:37:39   \n",
       "1  Mohon maaf atas ketidaknyamanan Bapak/Ibu, unt...  2025-04-08 07:37:38   \n",
       "2  Mohon maaf atas ketidaknyamanan Bapak/Ibu, unt...  2025-04-08 05:20:35   \n",
       "3  Terima kasih atas ulasannya. Semoga aplikasi B...  2025-04-08 05:06:44   \n",
       "4  Terima kasih atas ulasannya. Semoga aplikasi B...  2025-04-08 05:06:43   \n",
       "\n",
       "  appVersion  \n",
       "0      4.6.1  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4      4.6.1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/Meio047/Proyek-Analisis-Sentimen/refs/heads/main/bca_mobile_reviews.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rqMs1DSPYVpV",
    "outputId": "afe2be90-bbc6-4524-95d3-a6474696f383"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   reviewId              12000 non-null  object\n",
      " 1   userName              12000 non-null  object\n",
      " 2   userImage             12000 non-null  object\n",
      " 3   content               12000 non-null  object\n",
      " 4   score                 12000 non-null  int64 \n",
      " 5   thumbsUpCount         12000 non-null  int64 \n",
      " 6   reviewCreatedVersion  9629 non-null   object\n",
      " 7   at                    12000 non-null  object\n",
      " 8   replyContent          12000 non-null  object\n",
      " 9   repliedAt             12000 non-null  object\n",
      " 10  appVersion            9629 non-null   object\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 1.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eS8b1_kv1Vlg",
    "outputId": "acaf956a-40f8-4853-b9bc-0f1051e5c740"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12000, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0qoFX2Gc08vY"
   },
   "outputs": [],
   "source": [
    "df.drop(['reviewId', 'userName', 'userImage', 'score', 'thumbsUpCount',\n",
    "         'reviewCreatedVersion', 'at', 'replyContent', 'repliedAt', 'appVersion'],\n",
    "        axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOXnfpbcD5j8"
   },
   "source": [
    "Beberapa kolom diatas dihapus karena tidak diperlukan lagi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "v8pPhCDA1Gsl",
    "outputId": "d8837434-87e8-4f8f-ef55-cecf0d10c374"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bank terramah sepanjang masa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pulsa udah abis banyak buat verifikasi tapi ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kode otp tidak masuk masuk tolong perbaiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sangat puas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0                       bank terramah sepanjang masa\n",
       "1  pulsa udah abis banyak buat verifikasi tapi ke...\n",
       "2         kode otp tidak masuk masuk tolong perbaiki\n",
       "3                                        sangat puas\n",
       "4                                              Bagus"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJlwe9hbzDaJ"
   },
   "source": [
    "### Preprocesing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i1Qt8Reofg1Q"
   },
   "outputs": [],
   "source": [
    "def cleaningText(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'#[A-Za-z0-9]+', '', text)\n",
    "    text = re.sub(r'RT[\\s]', '', text)\n",
    "    text = re.sub(r\"http\\S+\", '', text)\n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    text = text.replace('\\n', ' ')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip(' ')\n",
    "    return text\n",
    "\n",
    "def casefoldingText(text): # Mengubah semua karakter dalam teks menjadi huruf kecil\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "def tokenizingText(text): # Memecah atau membagi string, teks menjadi daftar token\n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "def filteringText(text): # Menghapus stopwords dalam teks\n",
    "    listStopwords = set(stopwords.words('indonesian'))\n",
    "    listStopwords1 = set(stopwords.words('english'))\n",
    "    listStopwords.update(listStopwords1)\n",
    "    listStopwords.update(['iya','yaa','gak','nya','na','sih','ku',\"di\",\"ga\",\"ya\",\"gaa\",\"loh\",\"kah\",\"woi\",\"woii\",\"woy\"])\n",
    "    filtered = []\n",
    "    for txt in text:\n",
    "        if txt not in listStopwords:\n",
    "            filtered.append(txt)\n",
    "    text = filtered\n",
    "    return text\n",
    "\n",
    "def stemmingText(text): # Mengurangi kata ke bentuk dasarnya yang menghilangkan imbuhan awalan dan akhiran atau ke akar kata\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    stemmed_words = [stemmer.stem(word) for word in text]\n",
    "\n",
    "    stemmed_text = ' '.join(stemmed_words)\n",
    "\n",
    "    return stemmed_text\n",
    "\n",
    "def toSentence(list_words): # Mengubah daftar kata menjadi kalimat\n",
    "    sentence = ' '.join(word for word in list_words)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Mev-kVLnT_Ed"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://raw.githubusercontent.com/louisowen6/NLP_bahasa_resources/master/combined_slang_words.txt'\n",
    "# Fungsi untuk mengunduh dan memproses slang words\n",
    "def download_slangwords(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    lines = response.text.splitlines()\n",
    "    slang_dict = {}\n",
    "    for line in lines:\n",
    "        parts = line.split()\n",
    "        if len(parts) == 2:\n",
    "            slang_dict[parts[0].lower()] = parts[1]\n",
    "    return slang_dict\n",
    "\n",
    "slangwords = download_slangwords(url)\n",
    "\n",
    "def fix_slangwords(text):\n",
    "    words = text.split()\n",
    "    fixed_words = []\n",
    "\n",
    "    for word in words:\n",
    "        if word.lower() in slangwords:\n",
    "            fixed_words.append(slangwords[word.lower()])\n",
    "        else:\n",
    "            fixed_words.append(word)\n",
    "\n",
    "    fixed_text = ' '.join(fixed_words)\n",
    "    return fixed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "id": "ImSQY3BNqL6a",
    "outputId": "8028036b-3172-4f3d-ebe1-e44c32f5b46d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>text_casefoldingText</th>\n",
       "      <th>text_slangwords</th>\n",
       "      <th>text_tokenizingText</th>\n",
       "      <th>text_stopword</th>\n",
       "      <th>text_akhir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bank terramah sepanjang masa</td>\n",
       "      <td>bank terramah sepanjang masa</td>\n",
       "      <td>bank terramah sepanjang masa</td>\n",
       "      <td>bank terramah sepanjang masa</td>\n",
       "      <td>[bank, terramah, sepanjang, masa]</td>\n",
       "      <td>[bank, terramah]</td>\n",
       "      <td>bank terramah</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pulsa udah abis banyak buat verifikasi tapi ke...</td>\n",
       "      <td>pulsa udah abis banyak buat verifikasi tapi ke...</td>\n",
       "      <td>pulsa udah abis banyak buat verifikasi tapi ke...</td>\n",
       "      <td>pulsa udah abis banyak buat verifikasi tapi ke...</td>\n",
       "      <td>[pulsa, udah, abis, banyak, buat, verifikasi, ...</td>\n",
       "      <td>[pulsa, udah, abis, verifikasi, gabisa, pesana...</td>\n",
       "      <td>pulsa udah abis verifikasi gabisa pesana terba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kode otp tidak masuk masuk tolong perbaiki</td>\n",
       "      <td>kode otp tidak masuk masuk tolong perbaiki</td>\n",
       "      <td>kode otp tidak masuk masuk tolong perbaiki</td>\n",
       "      <td>kode otp tidak masuk masuk tolong perbaiki</td>\n",
       "      <td>[kode, otp, tidak, masuk, masuk, tolong, perba...</td>\n",
       "      <td>[kode, otp, masuk, masuk, tolong, perbaiki]</td>\n",
       "      <td>kode otp masuk masuk tolong perbaiki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sangat puas</td>\n",
       "      <td>sangat puas</td>\n",
       "      <td>sangat puas</td>\n",
       "      <td>sangat puas</td>\n",
       "      <td>[sangat, puas]</td>\n",
       "      <td>[puas]</td>\n",
       "      <td>puas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bagus</td>\n",
       "      <td>Bagus</td>\n",
       "      <td>bagus</td>\n",
       "      <td>bagus</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>[bagus]</td>\n",
       "      <td>bagus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0                       bank terramah sepanjang masa   \n",
       "1  pulsa udah abis banyak buat verifikasi tapi ke...   \n",
       "2         kode otp tidak masuk masuk tolong perbaiki   \n",
       "3                                        sangat puas   \n",
       "4                                              Bagus   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0                       bank terramah sepanjang masa   \n",
       "1  pulsa udah abis banyak buat verifikasi tapi ke...   \n",
       "2         kode otp tidak masuk masuk tolong perbaiki   \n",
       "3                                        sangat puas   \n",
       "4                                              Bagus   \n",
       "\n",
       "                                text_casefoldingText  \\\n",
       "0                       bank terramah sepanjang masa   \n",
       "1  pulsa udah abis banyak buat verifikasi tapi ke...   \n",
       "2         kode otp tidak masuk masuk tolong perbaiki   \n",
       "3                                        sangat puas   \n",
       "4                                              bagus   \n",
       "\n",
       "                                     text_slangwords  \\\n",
       "0                       bank terramah sepanjang masa   \n",
       "1  pulsa udah abis banyak buat verifikasi tapi ke...   \n",
       "2         kode otp tidak masuk masuk tolong perbaiki   \n",
       "3                                        sangat puas   \n",
       "4                                              bagus   \n",
       "\n",
       "                                 text_tokenizingText  \\\n",
       "0                  [bank, terramah, sepanjang, masa]   \n",
       "1  [pulsa, udah, abis, banyak, buat, verifikasi, ...   \n",
       "2  [kode, otp, tidak, masuk, masuk, tolong, perba...   \n",
       "3                                     [sangat, puas]   \n",
       "4                                            [bagus]   \n",
       "\n",
       "                                       text_stopword  \\\n",
       "0                                   [bank, terramah]   \n",
       "1  [pulsa, udah, abis, verifikasi, gabisa, pesana...   \n",
       "2        [kode, otp, masuk, masuk, tolong, perbaiki]   \n",
       "3                                             [puas]   \n",
       "4                                            [bagus]   \n",
       "\n",
       "                                          text_akhir  \n",
       "0                                      bank terramah  \n",
       "1  pulsa udah abis verifikasi gabisa pesana terba...  \n",
       "2               kode otp masuk masuk tolong perbaiki  \n",
       "3                                               puas  \n",
       "4                                              bagus  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Membersihkan teks dan menyimpannya di kolom 'text_clean'\n",
    "df['text_clean'] = df['content'].apply(cleaningText)\n",
    "# Mengubah huruf dalam teks menjadi huruf kecil dan menyimpannya di 'text_casefoldingText'\n",
    "df['text_casefoldingText'] = df['text_clean'].apply(casefoldingText)\n",
    "# Mengganti kata-kata slang dengan kata-kata standar dan menyimpannya di 'text_slangwords'\n",
    "df['text_slangwords'] = df['text_casefoldingText'].apply(fix_slangwords)\n",
    "# Memecah teks menjadi token (kata-kata) dan menyimpannya di 'text_tokenizingText'\n",
    "df['text_tokenizingText'] = df['text_slangwords'].apply(tokenizingText)\n",
    "# Menghapus kata-kata stop (kata-kata umum) dan menyimpannya di 'text_stopword'\n",
    "df['text_stopword'] = df['text_tokenizingText'].apply(filteringText)\n",
    "# Menggabungkan token-token menjadi kalimat dan menyimpannya di 'text_akhir'\n",
    "df['text_akhir'] = df['text_stopword'].apply(toSentence)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxxnRwpu0SJi"
   },
   "source": [
    "### Pelabelan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GQLQsdntURm3"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "from io import StringIO\n",
    "\n",
    "# Membaca data kamus kata-kata positif dari GitHub\n",
    "lexicon_positive = dict()\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_positive.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    # Membaca teks respons sebagai file CSV menggunakan pembaca CSV dengan pemisah koma\n",
    "\n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_positive[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata positif dan skornya ke dalam kamus lexicon_positive\n",
    "else:\n",
    "    print(\"Failed to fetch positive lexicon data\")\n",
    "\n",
    "# Membaca data kamus kata-kata negatif dari GitHub\n",
    "lexicon_negative = dict()\n",
    "\n",
    "response = requests.get('https://raw.githubusercontent.com/angelmetanosaa/dataset/main/lexicon_negative.csv')\n",
    "# Mengirim permintaan HTTP untuk mendapatkan file CSV dari GitHub\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Jika permintaan berhasil\n",
    "    reader = csv.reader(StringIO(response.text), delimiter=',')\n",
    "    for row in reader:\n",
    "        # Mengulangi setiap baris dalam file CSV\n",
    "        lexicon_negative[row[0]] = int(row[1])\n",
    "        # Menambahkan kata-kata negatif dan skornya dalam kamus lexicon_negative\n",
    "else:\n",
    "    print(\"Failed to fetch negative lexicon data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XBsWo5EnUUJK"
   },
   "outputs": [],
   "source": [
    "def sentiment_analysis_lexicon_indonesia(text):\n",
    "    score = 0\n",
    "\n",
    "    for word in text:\n",
    "\n",
    "        if (word in lexicon_positive):\n",
    "            score = score + lexicon_positive[word]\n",
    "\n",
    "    for word in text:\n",
    "\n",
    "        if (word in lexicon_negative):\n",
    "            score = score + lexicon_negative[word]\n",
    "\n",
    "    polarity=''\n",
    "    if score > 0:\n",
    "        polarity = 'positive'\n",
    "    elif score < 0:\n",
    "        polarity = 'negative'\n",
    "    else:\n",
    "        polarity = 'neutral'\n",
    "\n",
    "    return score, polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oIV2dRu9UYZQ",
    "outputId": "a3dd74b9-0ed3-4660-92ec-b11416b52db7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polarity\n",
      "negative    5459\n",
      "positive    4145\n",
      "neutral     2396\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "results = df['text_stopword'].apply(sentiment_analysis_lexicon_indonesia)\n",
    "results = list(zip(*results))\n",
    "df['polarity_score'] = results[0]\n",
    "df['polarity'] = results[1]\n",
    "print(df['polarity'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZAvIz2NSztYC"
   },
   "source": [
    "### Pembuatan model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tahapan\n",
    "1. Word2Vec Embedding -> Melatih representasi kata dari data sendiri\n",
    "2. Tokenisasi + Padding -> Mengubah teks menjadi format numerik untuk input neural network\n",
    "3. Model LSTM ->Menggunakan arsitektur Bi-LSTM untuk menangkap konteks kata\n",
    "4. Hyperparameter Tuning -> Meningkatkan performa model dengan pencarian parameter optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s1eilyVWcbL4",
    "outputId": "04c0b6cd-f70f-4e5e-ca2b-23cf3abc592c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==4.3.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gensim==4.3.3) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gensim==4.3.3) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from gensim==4.3.3) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim==4.3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "id": "KqThvYAJcSRW",
    "outputId": "38ff5a7a-f9a6-4f26-91f9-17b733543123"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2437\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "# Ambil data teks\n",
    "texts = df['text_akhir'].tolist()\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.split()  # Mengembalikan daftar token (kata)\n",
    "\n",
    "# Tokenisasi teks\n",
    "tokenized_texts = [tokenize_text(text) for text in df['text_akhir']]\n",
    "\n",
    "# Latih model Word2Vec\n",
    "model = Word2Vec(vector_size=300, window=5, min_count=3, workers=4)\n",
    "\n",
    "# Bangun vocabulary\n",
    "model.build_vocab(tokenized_texts)\n",
    "\n",
    "print(f\"Vocabulary size: {len(model.wv)}\")\n",
    "\n",
    "# Latih model\n",
    "model.train(tokenized_texts, total_examples=model.corpus_count, epochs=10)\n",
    "\n",
    "# Simpan model\n",
    "model.save(\"word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix sample:\n",
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.13103679  0.46979797  0.22473095 ... -0.10581052  0.58515042\n",
      "   0.08734687]\n",
      " [-0.07539558  0.23952591 -0.02895861 ... -0.19566475  0.30429947\n",
      "  -0.18140358]\n",
      " [-0.19879891  0.09817228 -0.25772256 ... -0.19310902  0.19679204\n",
      "  -0.31252772]\n",
      " [-0.31022692 -0.34537435 -0.50013959 ... -0.17876725 -0.02901619\n",
      "  -0.23413682]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "model = Word2Vec.load(\"word2vec_model.model\")\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['text_akhir'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text_akhir'])\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Buat matriks embedding\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_dim = model.vector_size  \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if word in model.wv:\n",
    "        embedding_matrix[i] = model.wv[word]\n",
    "\n",
    "print(\"Embedding matrix sample:\")\n",
    "print(embedding_matrix[:5])\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(df['text_akhir'])\n",
    "max_length = max(len(seq) for seq in X_sequences)\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_encoded = label_encoder.fit_transform(df['polarity'])\n",
    "\n",
    "# Memecah data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_padded, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras-tuner --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 01m 29s]\n",
      "val_accuracy: 0.7833333611488342\n",
      "\n",
      "Best val_accuracy So Far: 0.9083333611488342\n",
      "Total elapsed time: 00h 18m 04s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 24 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9043 - loss: 0.3147\n",
      "Test Accuracy: 0.9117\n"
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Konversi label ke one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_cat = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test_cat = to_categorical(y_test, num_classes=num_classes)\n",
    "\n",
    "# Function untuk membangun model\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=max_length,\n",
    "        trainable=hp.Boolean(\"trainable_embedding\")\n",
    "    ))\n",
    "    model.add(Bidirectional(LSTM(\n",
    "        units=hp.Int('lstm_units', min_value=32, max_value=128, step=32),\n",
    "        return_sequences=False\n",
    "    )))\n",
    "    model.add(Dropout(hp.Float('dropout_rate', 0.2, 0.6, step=0.1)))\n",
    "    model.add(Dense(\n",
    "        units=hp.Int('dense_units', 32, 128, step=32),\n",
    "        activation='relu'\n",
    "    ))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Choice('learning_rate', [1e-3, 5e-4, 1e-4])),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Inisialisasi tuner\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='my_tuner_dir',\n",
    "    project_name='word2vec_lstm_tuning'\n",
    ")\n",
    "\n",
    "# Callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Mulai tuning\n",
    "tuner.search(X_train, y_train_cat,\n",
    "             epochs=20,\n",
    "             validation_split=0.2,\n",
    "             callbacks=[early_stop],\n",
    "             verbose=1)\n",
    "\n",
    "# Ambil model terbaik\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Evaluasi model terbaik\n",
    "loss, accuracy = best_model.evaluate(X_test, y_test_cat)\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'neutral', 'positive'], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_to_predict = [\n",
    "    \"Produk ini biasa saja\", \n",
    "    \"Sangat memuaskan, saya suka sekali!\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs = tokenizer.texts_to_sequences(texts_to_predict)\n",
    "padded_seqs = pad_sequences(seqs, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 330ms/step\n",
      "Teks: \"Produk ini biasa saja\" → Prediksi: neutral\n",
      "Teks: \"Sangat memuaskan, saya suka sekali!\" → Prediksi: positive\n"
     ]
    }
   ],
   "source": [
    "pred_probs = best_model.predict(padded_seqs)\n",
    "\n",
    "pred_classes = pred_probs.argmax(axis=1)\n",
    "\n",
    "# Konversi ke label kategori\n",
    "predicted_labels = label_encoder.inverse_transform(pred_classes)\n",
    "\n",
    "# Tampilkan hasil\n",
    "for text, label in zip(texts_to_predict, predicted_labels):\n",
    "    print(f\"Teks: \\\"{text}\\\" → Prediksi: {label}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
